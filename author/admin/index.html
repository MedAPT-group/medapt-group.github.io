<!doctype html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: December 16, 2025 --><html lang="en-us" dir="ltr"
      data-wc-theme-default="system">
  
<head><script src="/new_page/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=new_page/livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="generator" content="Hugo Blox Builder 0.2.0" />

  
  












  
  
  
  
  
  

  

  
  
  
    
  
  <meta name="description" content="The highly-customizable Hugo Academic theme powered by Hugo Blox Builder. Easily create your personal academic website." />

  
  <link rel="alternate" hreflang="en-us" href="http://localhost:1313/new_page/author/admin/" />

  
  
  
  
    
    <link rel="stylesheet" href="/new_page/css/themes/blue.min.css" />
  

  
  
    
    <link href="/new_page/dist/wc.min.css" rel="stylesheet" />
  

  
  
  

  

  <script>
     
    window.hbb = {
       defaultTheme: document.documentElement.dataset.wcThemeDefault,
       setDarkTheme: () => {
        document.documentElement.classList.add("dark");
        document.documentElement.style.colorScheme = "dark";
      },
       setLightTheme: () => {
        document.documentElement.classList.remove("dark");
        document.documentElement.style.colorScheme = "light";
      }
    }

    console.debug(`Default Hugo Blox Builder theme is ${window.hbb.defaultTheme}`);

    if ("wc-color-theme" in localStorage) {
      localStorage.getItem("wc-color-theme") === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
    } else {
      window.hbb.defaultTheme === "dark" ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      if (window.hbb.defaultTheme === "system") {
        window.matchMedia("(prefers-color-scheme: dark)").matches ? window.hbb.setDarkTheme() : window.hbb.setLightTheme();
      }
    }
  </script>

  <script>
    
    document.addEventListener('DOMContentLoaded', function () {
      
      let checkboxes = document.querySelectorAll('li input[type=\'checkbox\'][disabled]');
      checkboxes.forEach(e => {
        e.parentElement.parentElement.classList.add('task-list');
      });

      
      const liNodes = document.querySelectorAll('.task-list li');
      liNodes.forEach(nodes => {
        let textNodes = Array.from(nodes.childNodes).filter(node => node.nodeType === 3 && node.textContent.trim().length > 1);
        if (textNodes.length > 0) {
          const span = document.createElement('label');
          textNodes[0].after(span);  
          span.appendChild(nodes.querySelector('input[type=\'checkbox\']'));
          span.appendChild(textNodes[0]);
        }
      });
    });
  </script>

  
  
  




































  
  
    <link rel="alternate" href="/new_page/author/admin/index.xml" type="application/rss+xml" title="Xia Li" />
  

  
  <link rel="icon" type="image/png" href="/new_page/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/new_page/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="http://localhost:1313/new_page/author/admin/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
    <meta property="twitter:site" content="@GetResearchDev" />
    <meta property="twitter:creator" content="@GetResearchDev" />
  
  <meta property="og:site_name" content="Xia Li" />
  <meta property="og:url" content="http://localhost:1313/new_page/author/admin/" />
  <meta property="og:title" content="Admin | Xia Li" />
  <meta property="og:description" content="The highly-customizable Hugo Academic theme powered by Hugo Blox Builder. Easily create your personal academic website." /><meta property="og:image" content="http://localhost:1313/new_page/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="http://localhost:1313/new_page/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta property="og:updated_time" content="2024-11-09T00:00:00&#43;00:00" />
    
  

  




  <title>Admin | Xia Li</title>

  
  
  
  
  
    
    
  
  
  <style>
    @font-face {
      font-family: 'Inter var';
      font-style: normal;
      font-weight: 100 900;
      font-display: swap;
      src: url(/new_page/dist/font/Inter.var.woff2) format(woff2);
    }
  </style>

  

  
  


  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
  
  
  
  
  
  
  
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
        
        
          
        
          
        
          
        
          
        
          
        
        
        
      
    
      
      
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  







<link type="text/css" rel="stylesheet" href="/new_page/dist/pagefind/pagefind-ui.be766eb419317a14ec769d216e9779bfe8f3737c80e780f4ba0dafb57a41a482.css" integrity="sha256-vnZutBkxehTsdp0hbpd5v&#43;jzc3yA54D0ug2vtXpBpII=" />


<script src="/new_page/dist/pagefind/pagefind-ui.87693d7c6f2b3b347ce359d0ede762c033419f0a32b22ce508c335a81d841f1b.js" integrity="sha256-h2k9fG8rOzR841nQ7ediwDNBnwoysizlCMM1qB2EHxs="></script>


<script>window.hbb.pagefind = {"baseUrl":"/new_page/"};</script>

<style>
  html.dark {
    --pagefind-ui-primary: #eeeeee;
    --pagefind-ui-text: #eeeeee;
    --pagefind-ui-background: #152028;
    --pagefind-ui-border: #152028;
    --pagefind-ui-tag: #152028;
  }
</style>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    new PagefindUI({
      element: "#search",
      showSubResults: true,
      baseUrl: window.hbb.pagefind.baseUrl,
      bundlePath: window.hbb.pagefind.baseUrl + "pagefind/",
    });
  });
  document.addEventListener('DOMContentLoaded', () => {
    let element = document.getElementById('search');
    let trigger = document.getElementById('search_toggle');

    if (trigger) {
      trigger.addEventListener('click', () => {
        element.classList.toggle('hidden');
        element.querySelector("input").value = ""
        element.querySelector("input").focus()

        if (!element.classList.contains('hidden')) {
          let clear_trigger = document.querySelector('.pagefind-ui__search-clear');

          if (clear_trigger && !clear_trigger.hasAttribute('listenerOnClick')) {
            clear_trigger.setAttribute('listenerOnClick', 'true');

            clear_trigger.addEventListener('click', () => {
              element.classList.toggle('hidden');
            });
          }
        }

      });
    }
  });
</script>















  
  
  
  
  
  
  
  <script
    defer
    src="/new_page/js/hugo-blox-en.min.js"
    integrity=""
  ></script>

  
  








  
    
      
      <script async defer src="https://buttons.github.io/buttons.js"></script>

      
    
  




</head>

  <body class="dark:bg-hb-dark dark:text-white page-wrapper" id="top">
    <div id="page-bg"></div>
    <div class="page-header sticky top-0 z-30">
      
      
      
        
        
        
          <header id="site-header" class="header">
  <nav class="navbar px-3 flex justify-left">
    <div class="order-0 h-100">
      
      <a class="navbar-brand" href="/new_page/" title="Xia Li">
        Xia Li
      </a>
    </div>
    
    <input id="nav-toggle" type="checkbox" class="hidden" />
    <label
      for="nav-toggle"
      class="order-3 cursor-pointer flex items-center lg:hidden text-dark dark:text-white lg:order-1">
      <svg id="show-button" class="h-6 fill-current block" viewBox="0 0 20 20">
        <title>Open Menu</title>
        <path d="M0 3h20v2H0V3z m0 6h20v2H0V9z m0 6h20v2H0V0z"></path>
      </svg>
      <svg id="hide-button" class="h-6 fill-current hidden" viewBox="0 0 20 20">
        <title>Close Menu</title>
        <polygon
          points="11 9 22 9 22 11 11 11 11 22 9 22 9 11 -2 11 -2 9 9 9 9 -2 11 -2"
          transform="rotate(45 10 10)"></polygon>
      </svg>
    </label>
    

    
    
    <ul
      id="nav-menu"
      class="navbar-nav order-3 hidden lg:flex w-full pb-6 lg:order-1 lg:w-auto lg:space-x-2 lg:pb-0 xl:space-x-8 justify-left
      ">
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/new_page/"
        >Home</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/new_page/#news"
        >News</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/new_page/publication"
        >Publications</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/new_page/authors"
        >Group</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/new_page/event"
        >Talks</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/new_page/experience/"
        >Experience</a
        >
      </li>
      
      
      
      
      
      
      <li class="nav-item">
        <a
          class="nav-link "
          
          href="/new_page/awards/"
        >Awards</a
        >
      </li>
      
      
      
    </ul>

    <div class="order-1 ml-auto flex items-center md:order-2 lg:ml-0">

      
      
      
      <button
        aria-label="search"
        class="text-black hover:text-primary  inline-block px-3 text-xl dark:text-white"
        id="search_toggle">
        <svg xmlns="http://www.w3.org/2000/svg" height="16" width="16" viewBox="0 0 512 512" fill="currentColor"><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352a144 144 0 1 0 0-288 144 144 0 1 0 0 288z"/></svg>
      </button>
      

      
      
      <div class="px-3 text-black hover:text-primary-700 dark:text-white dark:hover:text-primary-300
            [&.active]:font-bold [&.active]:text-black/90 dark:[&.active]:text-white">
        <button class="theme-toggle mt-1" accesskey="t" title="appearance">
          <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class="dark:hidden">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
          <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
               fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
               stroke-linejoin="round" class=" dark:block [&:not(dark)]:hidden">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
          </svg>
        </button>
      </div>
      

      
      

      
      
    </div>
  </nav>
</header>


<div id="search" class="hidden p-3"></div>


        
      
    </div>
    <div class="page-body  my-10">
      

<div class="max-w-prose mx-auto flex justify-center">
  <article class="prose prose-slate lg:prose-xl dark:prose-invert">
    <h1 class="lg:text-6xl">Admin</h1>
    
  </article>
</div>




<div class="flex flex-col items-center">

  <div class="container max-w-[65ch] mx-auto bg-white dark:bg-zinc-900 rounded-xl border-gray-100 dark:border-gray-700 border shadow-md overflow-hidden my-5">


  
  
  








<a href="/new_page/publication/arxiv24-gaussian-dir/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/arxiv24-gaussian-dir/featured_hud0a95f438efb6037197997b6e620f89a_390284_84c3b790c999ffb14dec4c3bc76d40e3.webp" height="655" width="655" alt="Gaussian Representation for Deformable Image Registration">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Gaussian Representation for Deformable Image Registration</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        This paper presented a novel deformable image registration (DIR) technique using parametric 3D Gaussian control points to model continuous spatial deformation fields effectively. By integrating transformation vectors with linear blend skinning (LBS) for voxel motion interpolation, our approach simplifies computational demands while ensuring high accuracy and efficiency. The adaptive density of Gaussian points further allows for precise handling of varying motion complexities.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Nov 9, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/arxiv24-cpt-interp/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/arxiv24-cpt-interp/featured_hu15a5c9af63489bcaf226aed92a1a51e6_218107_f4e451e28d0be8880d96ab4cf69104af.webp" height="655" width="655" alt="CPT-Interp: Continuous sPatial and Temporal Motion Modeling for 4D Medical Image Interpolation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">CPT-Interp: Continuous sPatial and Temporal Motion Modeling for 4D Medical Image Interpolation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        In this work, we introduce a novel Continuous sPatial and Temporal (CPT) modeling approach for 4D medical image interpolation, addressing the limitations of discrete motion modeling in both spatial and temporal domains. By leveraging implicit neural representations and integrating an ODE solver, our method achieves superior accuracy and speed without requiring extensive training datasets or domain-specific fine-tuning. Through comprehensive experiments and ablation studies, we demonstrated the contributions of spatial and temporal continuity, as well as the robustness of our method across different datasets. CPT-Interp not only outperforms state-of-the-art methods but also exhibits remarkable generalization ability, making it a promising solution for clinical applications.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Nov 9, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/event/iccr24-rising-star/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/event/iccr24-rising-star/featured_hu59f0b7d5d27aead8605a8cb7307212bd_54457_3f780e40b58ca2ebc20c6c4e225c558b.webp" height="655" width="655" alt="ICCR 2024 Rising Star Competition">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">ICCR 2024 Rising Star Competition</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        Rising Star Competition by WIMP</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jul 10, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/iccr24-ngp-dir/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/iccr24-ngp-dir/featured_hu96fe106805ab412538f27a974e3cf4c4_377606_2a234543f044e8f08f88faac538f473d.webp" height="655" width="655" alt="Neural Graphics Primitives-based Deformable Image Registration for On-the-fly Motion Extraction">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Neural Graphics Primitives-based Deformable Image Registration for On-the-fly Motion Extraction</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        In this study, we have successfully integrated NGP into DIR, a novel contribution that significantly enhances the accuracy and efficiency of medical image alignment as demonstrated on the DIR-lab dataset. The NGPDIR framework exhibits robust performance across various metrics, particularly in landmark alignment precision and the accommodation of anatomical sliding boundaries. This advancement not only propels the DIR field forward but also opens new avenues for real-time clinical applications, potentially transforming patient care with its rapid, reliable imaging capabilities.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jul 8, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/arxiv24-tam-bridge-slam/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/arxiv24-tam-bridge-slam/featured_hu0cd862289b315b60807aff68793c0a8e_733908_bcbeae00b74db71165e08454cbd26f68.webp" height="655" width="655" alt="TAMBRIDGE: Bridging Frame-Centered Tracking and 3D Gaussian Splatting for Enhanced SLAM">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">TAMBRIDGE: Bridging Frame-Centered Tracking and 3D Gaussian Splatting for Enhanced SLAM</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        Our goal was to develop a 3DGS-based SLAM system suitable for intelligent robotic perception tasks. The system is designed to meet the real-time requirements of robotic perception, and is robust against random sensor noise, motion blur and challenges posed by long-session SLAM. To achieve this, we have implemented a plug-and-play Fusion Bridge module that filters redundant views and leverages the anchoring effect of reprojection errors to estimate poses for rendering initially. This approach marks the first integration of a tracking-centered paradigm with an ORB-based visual odometry frontend together with a 3DGS-centered backend. Our system consistently achieves realtime localization and near-real-time rendering at over 5 FPS on the real-world TUM RGB-D dataset. Furthermore, it significantly surpasses SplaTAM in localization accuracy, rendering quality, and robustness in scenarios involving motion blur and long-distance SLAM tasks</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">May 30, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/event/estro24-proffered-paper/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/event/estro24-proffered-paper/featured_hueaa20a60304dfba750d2a2c292d27e84_338124_a4d0e89aff74faa59dbadb8bf012df21.webp" height="655" width="655" alt="ESTRO 2024 Proffered Paper">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">ESTRO 2024 Proffered Paper</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        Proffered paper presentation for continuous deformable image registration</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">May 4, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/estro24-cpt-dir/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/estro24-cpt-dir/featured_hu021ed6cb546139d62aea82ab3c1fde08_97314_04edab3e89aa5b4909b1123ce047e340.webp" height="655" width="655" alt="Beyond Voxel-Based Methods: Continuous Motion Modeling for Enhanced Deformable Image Registration">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Beyond Voxel-Based Methods: Continuous Motion Modeling for Enhanced Deformable Image Registration</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        Our novel approach, capitalizing on the power of INR, offers a means to model motion in a continuous manner for intra-fraction motion modelling. By representing the registration process as a continuous flow, we circumvent the limitations inherent to the classic voxel-based representations, particularly the dilemma of discretization. The tangible enhancements are evident: both INR-DIR (E2E and LDD) methods consistently outperformed classic B-splines regarding TRE, MAE, landmark, and Dice coefficients. Remarkably, the registration times were slashed by more than half. One of the standout features of our LDD method is its ability to handle reverse trajectories, a capability unattainable with prior techniques. This promises more robust and versatile image registration applications in radiotherapy.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">May 3, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/phiro24-cpt-dir/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/phiro24-cpt-dir/featured_huc14d0a5fa40c5cfc1754c530f86165ca_342572_3e84b9476d787197991273461062fbb7.webp" height="655" width="655" alt="Continuous sPatial-Temporal Deformable Image Registration (CPT-DIR) for motion modelling in radiotherapy: beyond classic voxel-based methods">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Continuous sPatial-Temporal Deformable Image Registration (CPT-DIR) for motion modelling in radiotherapy: beyond classic voxel-based methods</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        In summary, the innovative CPT-DIR approach, integrating principles of INR and LDDMM, represents a11 significant departure from traditional voxel-based methods in DIR. By adopting a paradigm of continuous motion modelling, we transcend the limitations inherent in voxel-based representations, offering a more robust, automatic and versatile solution. Leveraging spatial continuity, we effectively handle the intricacies of sliding organ boundaries, while temporal continuity alleviates the complexities associated with significant anatomical changes over time. The tangible benefits are evident in its superior performance compared to classic B-splines methods. CPT-DIR consistently achieves better performance by all kinds of evaluation matrices. Additionally, the efficiency gains are substantial, with registration times slashed by more than half.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">May 1, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/tip24-ref-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/tip24-ref-seg/featured_hu2be32ba41f2b067d2f7691bdca28cd05_540168_5ced645f07f340125b37c6d45aa7d395.webp" height="655" width="655" alt="Towards robust referring image segmentation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Towards robust referring image segmentation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We propose a novel ranking loss function, named Bi-directional Exponential Angular Triplet Loss, to help learn an angularly separable common feature space by explicitly constraining the included angles between embedding vectors.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Mar 5, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/arxiv24-uncert-tto-pose/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/arxiv24-uncert-tto-pose/featured_hu9c494abd5f76bf38872a6dbd835fa19d_118276_199a7bdd68dd149610a47cb4a6758662.webp" height="655" width="655" alt="Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        In this paper, we propose an Uncertainty-Aware testing-time Optimization (UAO) framework for 3D human pose estimation. During the training process, we propose the GUMLP to estimate 3D results and uncertainty values for each joint. For test-time optimization, our UAO framework freezes the pre-trained network parameters and optimizes a latent state initialized by the input 2D pose. To constrain the optimization direction in both 2D and 3D spaces, projection and uncertainty constraints are applied. Extensive experiments show that our approach achieves state-of-the-art performance on two popular datasets</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Feb 15, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/tpami24-open-voc-learn/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/tpami24-open-voc-learn/featured_hu8ee485cd77c3ebd2b69ba43ec7c9cc60_155191_6f7a44290f36ad698ba098a932e1521b.webp" height="655" width="655" alt="Towards open vocabulary learning: A survey">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Towards open vocabulary learning: A survey</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        This survey offers a detailed examination of the latest developments in open vocabulary learning in computer vision, which appears to be a first of its kind. We provide an overview of the necessary background knowledge, which includes fundamental concepts and introductory knowledge of detection, segmentation, and vision language pre-training. Following that, we summarize more than 50 different models used for various scene understanding tasks. For each task, we categorize the methods based on their technical viewpoint. Additionally, we provide information regarding several closely related domains. In the experiment section, we provide a detailed description of the settings and compare results fairly. Finally, we summarize several challenges and also point out several future research directions for open vocabulary learning.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Feb 5, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/ro23-uncert-mr2ct/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/ro23-uncert-mr2ct/featured_hu1e82422065b28e5b05443b600c8648df_1667560_22d2db3fe032dc31a17abc0463ba96aa.webp" height="655" width="655" alt="Uncertainty-aware MR-based CT synthesis for robust proton therapy planning of brain tumour">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Uncertainty-aware MR-based CT synthesis for robust proton therapy planning of brain tumour</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        The enhanced framework incorporates 3D uncertainty prediction and generates high-quality sCTs from MR images. The framework also facilitates conditioned robust optimisation, bolstering proton plan robustness against network prediction errors. The innovative feature of uncertainty visualisation and robust analyses contribute to evaluating sCT clinical utility for individual patients.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Feb 1, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/medphys24-joint-gen-reg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/medphys24-joint-gen-reg/featured_hu36e7445b32da9f0beb25fa32f2b4d30d_137209_38b9ab817556aae6df39a3a95dca2555.webp" height="655" width="655" alt="A Unified Generation-Registration Framework for Improved MR-based CT Synthesis in Proton Therapy">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">A Unified Generation-Registration Framework for Improved MR-based CT Synthesis in Proton Therapy</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        In this study, we developed an approach that substantially improves the anatomical accuracy of synthetic CT (sCT) images for MR-based radiotherapy planning. By integrating image generation with the deformable registration processes, the novel framework effectively addresses the inherent challenges in MR-CT registration and sCT evaluation. The methodological synergy of generation and registration networks, coupled with a strategic approach to proton dose planning, contributes to enhancing treatment planning accuracy. This study represents a meaningful step forward in the application of MR-based proton treatment planning and daily adaptive replanning, offering potential improvements in reducing the imaging-related dose and patient outcomes in the field of radiotherapy.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jan 20, 2024</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/post/23-12-18-estro24-proffered-paper/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/post/23-12-18-estro24-proffered-paper/featured_hu2267e437701dbad5bb73230e2d74f650_40744_1474d2f87f2e8143a2663f99ac5f6669.webp" height="655" width="655" alt="Our abstract is selected as a Proffered Paper by ESTRO 2024!">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Our abstract is selected as a Proffered Paper by ESTRO 2024!</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        Beyond Voxel-Based Methods: Continuous Motion Modeling for Enhanced Deformable Image Registration</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Dec 18, 2023</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/cvpr24-icl-pose/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/cvpr24-icl-pose/featured_hu4d9ec96548c27c65612f16aed24d56cd_238333_d232fe1d3bb9f617c13e9a4b9faed1db.webp" height="655" width="655" alt="Skeleton-in-context: Unified skeleton sequence modeling with in-context learning">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Skeleton-in-context: Unified skeleton sequence modeling with in-context learning</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        In this work, we propose the Skeleton-in-Context, designed to process multiple skeleton-base tasks simultaneously after just one training time. Specifically, we build a novel skeleton-based in-context benchmark covering various tasks. In particular, we propose skeleton prompts composed of TGP and TUP, which solve the overfitting problem of skeleton sequence data trained under the training framework commonly applied in previous 2D and 3D in-context models. Besides, we demonstrate that our model can generalize to different datasets and new tasks, such as motion completion. We hope our research builds the first step in the exploration of in-context learning for skeleton-based sequences, which paves the way for further research in this area.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Dec 15, 2023</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/neurips23-icl-point/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/neurips23-icl-point/featured_huc09ae552efe385e96289ecf777809b96_472423_0c92884518cfab477d5947240d03462f.webp" height="655" width="655" alt="Explore In-Context Learning for 3D Point Cloud Understanding">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Explore In-Context Learning for 3D Point Cloud Understanding</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We propose Point-In-Context (PIC), the first framework adopting the in-context learning paradigm for 3D point cloud understanding. Specifically, we set up an extensive dataset of point cloud pairs with four fundamental tasks to achieve in-context ability. We propose effective designs that facilitate the training and solve the inherited information leakage problem. PIC shows its excellent learning capacity, achieves comparable results with single-task models, and outperforms multitask models on all four tasks. Besides, it shows good generalization ability to out-of-distribution samples and unseen tasks and has great potential via selecting higher-quality prompts. We hope it paves the way for further exploration of in-context learning in the 3D modalities.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Dec 10, 2023</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/iccv23-coevo-mesh/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/iccv23-coevo-mesh/featured_hu6360c533b6c06cede09efa83e846afd5_447307_1b13ee32298dc18d92fd51212346c948.webp" height="655" width="655" alt="Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        This paper proposes the Pose and Mesh Co-Evolution network (PMCE), a new two-stage pose-to-mesh framework for recovering 3D human mesh from a monocular video. PMCE frst estimates 3D human pose motion in terms of spatial and temporal domains, then performs image-guided pose and mesh interactions by our proposed AdaLN that injects body shape information while preserving their spatial structure. Extensive experiments on popular datasets show that PMCE outperforms state-of-the-art methods in both perframe accuracy and temporal consistency. We hope that our approach will spark further research in 3D human motion estimation considering both pose and shape consistency.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Oct 2, 2023</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/iccv23-caption-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/iccv23-caption-seg/featured_hu011ea7685f847792f4a9d7cc2f95615e_389155_22c77cfa92b09631b702288094ee7bc9.webp" height="655" width="655" alt="Betrayed by captions: Joint caption grounding and generation for open vocabulary instance segmentation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Betrayed by captions: Joint caption grounding and generation for open vocabulary instance segmentation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        This paper presents a joint Caption Grounding and Generation (CGG) framework for instance-level open vocabulary segmentation. The main contributions are: (1) using fine-grained object nouns in captions to improve grounding with object queries. (2) using captions as supervision signals to extract rich information from other words helps identify novel categories. To our knowledge, this paper is the first to unify segmentation and caption generation for open vocabulary learning. The proposed framework significantly improves OVIS and OSPS and comparable results on OVOD without pre-training on large-scale datasets.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Oct 2, 2023</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/event/estro23-mini-oral/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/event/estro23-mini-oral/featured_hucbeeb0ee2df3523c7003bb7a14e993a7_177126_663f6cbacb74b71cc6ef92414c04ee0c.webp" height="655" width="655" alt="ESTRO 2023 Mini-Oral">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">ESTRO 2023 Mini-Oral</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        Mini-oral presentation for uncertainty-conditioned MR-guided proton therapy</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">May 13, 2023</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/post/22-12-20-estro23-mini-oral/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/post/22-12-20-estro23-mini-oral/featured_hu2267e437701dbad5bb73230e2d74f650_40744_1474d2f87f2e8143a2663f99ac5f6669.webp" height="655" width="655" alt="Our abstract is selected as a Mini-Oral presentation by ESTRO 2023!">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Our abstract is selected as a Mini-Oral presentation by ESTRO 2023!</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        Uncertainty-aware MR-base CT synthesis for robust proton planning of skull-based tumour</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Dec 20, 2022</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/tpami22-equil-learn/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/tpami22-equil-learn/featured_hu10b75be164534ddad67407bc06770508_311549_8acdbffbe2c23835495c3336c748776d.webp" height="655" width="655" alt="Optimization induced equilibrium networks: An explicit optimization perspective for understanding equilibrium models">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Optimization induced equilibrium networks: An explicit optimization perspective for understanding equilibrium models</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        In this paper, we decompose the feed-forward DNN and find a more reasonable basic unit layer, which shows a close relationship with the proximal operator. Based on it, we propose new equilibrium models, OptEqs, and explore their underlying optimization problems thoroughly. We provide two strategies to introduce customized regularizations to the equilibrium points, and achieve significant performance improvement in experiments. We highlight that by modifying the underlying optimization problems, we can create more effective network architectures. Our work may inspire more interpretable equilibrium models from the optimization perspective.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jun 10, 2022</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/pr21-fashion-pose/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/pr21-fashion-pose/featured_hu7c3454da90b2208136f3bb2fcd0b96e9_189885_bcf1fd78f33f97f2d6f8526b7681b144.webp" height="655" width="655" alt="PCLoss: Fashion landmark estimation with position constraint loss">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">PCLoss: Fashion landmark estimation with position constraint loss</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        In this paper, we design a Position Constraint Loss (PCLoss) for fashion landmark estimation, which incorporates the position correlation into landmark estimation models. Specifically, the PCLoss adds a regular term for each landmark to regularize their relative positions. Compared with other alternatives, our PCLoss effectively mitigates the outliers and duplicate detection problems without modifying existing CNN architectures. In addition, our skeleton-like optimization method further strengthens the position constraints between landmarks. The proposed method can be applied to both regression and heatmap based methods and it provides a novel perspective towards position relation learning in key point estimation tasks. Extensive experimental results on three challenging datasets, DeepFashion, FLD and FashionAI, demonstrate that our method outperforms other state-of-the-art methods. The experiment on COCO 2017 shows the potential applications of PCLoss for other key point estimation tasks, which can be explored more in future work.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Oct 1, 2021</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/neurips21-proto-track-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/neurips21-proto-track-seg/featured_hucb35216e12caf65b285e8177fe5d0540_381581_3b16783e3fb1a58fcc4311f346ab2bd8.webp" height="655" width="655" alt="Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We propose Prototypical Cross-Attention Network (PCAN), capable of leveraging rich spatio-temporal information for online multiple object tracking and segmentation. </p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Sep 12, 2021</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/tip21-sqreason-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/tip21-sqreason-seg/featured_hub67fae09e3f86c7671a6a91178fc4522_263697_da8b60ef7e685749194388c21ac508de.webp" height="655" width="655" alt="Towards efficient scene understanding via squeeze reasoning">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Towards efficient scene understanding via squeeze reasoning</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        </p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jul 30, 2021</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/cvpr21-quasi-track/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/cvpr21-quasi-track/featured_huf1361c5b795c8badd6f2979bf92e6920_1106383_8f05b824185fefba4ef9c099edf5ead6.webp" height="655" width="655" alt="Quasi-dense similarity learning for multiple object tracking">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Quasi-dense similarity learning for multiple object tracking</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We present Quasi-Dense Similarity Learning, which densely samples hundreds of region proposals on a pair of images for contrastive learning.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Feb 20, 2021</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/cvpr21-point-flow-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/cvpr21-point-flow-seg/featured_hu309c9fb14b8abc1ebff95577a488bf34_593424_06f8df259df6fba4b00a52b4acb9163a.webp" height="655" width="655" alt="PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        </p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Feb 13, 2021</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/tip20-atloss-reid/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/tip20-atloss-reid/featured_hu63751962ff0fad11a5926e71a1a0c7e2_179116_df3552ae681d53479e7fea365cdd6aea.webp" height="655" width="655" alt="Bi-directional Exponential Angular Triplet Loss for RGB-Infrared Person Re-Identification">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Bi-directional Exponential Angular Triplet Loss for RGB-Infrared Person Re-Identification</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We propose a novel ranking loss function, named Bi-directional Exponential Angular Triplet Loss, to help learn an angularly separable common feature space by explicitly constraining the included angles between embedding vectors.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Dec 12, 2020</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/iclr20-md-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/iclr20-md-seg/featured_hue037f6df33459d6147d3b335be2799d9_74977_fc088931a148c4104916f66071749b6b.webp" height="655" width="655" alt="Is Attention Better Than Matrix Decomposition?">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Is Attention Better Than Matrix Decomposition?</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        Self-attention is not better than the matrix decomposition~(MD) model developed 20 years ago regarding the performance and computational cost for encoding the long-distance dependencies.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Sep 28, 2020</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/eccv20-decouple-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/eccv20-decouple-seg/featured_huc6030c583a6246ba24d8ca663433c0d8_604507_7cff5f71aed8efed1785aced0f7540e4.webp" height="655" width="655" alt="Improving Semantic Segmentation via Decoupled Body and Edge Supervision">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Improving Semantic Segmentation via Decoupled Body and Edge Supervision</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        </p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jul 3, 2020</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/cvpr20-spygr-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/cvpr20-spygr-seg/featured_hu43d2a3705a01344ae35e6916ebaa3685_689664_581ba5760dba1162fb3718d933d4687d.webp" height="655" width="655" alt="Spatial Pyramid Based Graph Reasoning for Semantic Segmentation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Spatial Pyramid Based Graph Reasoning for Semantic Segmentation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        </p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Feb 24, 2020</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/aaai20-sognet-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/aaai20-sognet-seg/featured_hu559f28cda505009a79e68e1e21f4a288_271305_fec9a3b8d7933a278d52e017fb506aeb.webp" height="655" width="655" alt="SOGNet: Scene Overlap Graph Network for Panoptic Segmentation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">SOGNet: Scene Overlap Graph Network for Panoptic Segmentation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We leverage each objects category, geometry and appearance features to perform relational embedding, and output a relation matrix that encodes overlap relations. In order to overcome the lack of supervision, we introduce a differentiable module to resolve the overlap between any pair of instances.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Feb 7, 2020</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/aaai20-tcs-cls/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/aaai20-tcs-cls/featured_huaf3a0810c4a3c1ca5a417386bed61ce5_46007_0621976457504149618aaa1bdb321f6b.webp" height="655" width="655" alt="Dynamic System Inspired Adaptive Time Stepping Controller for Residual Networks Families">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Dynamic System Inspired Adaptive Time Stepping Controller for Residual Networks Families</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We analyze the effects of time stepping on the Euler method and ResNets. We establish a stability condition for ResNets with step sizes and weight parameters, and point out the effects of step sizes on the stability and performance. Inspired by our analyses, we develop an adaptive time stepping controller that is dependent on the parameters of the current step, and aware of previous steps.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Feb 7, 2020</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/event/iccv19-oral/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/event/iccv19-oral/featured_hu545592b4f28d304f81b32973adb708ca_63558_7efc57fcc663c1e6a8932b53d0d07eec.webp" height="655" width="655" alt="ICCV 2019 Oral">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">ICCV 2019 Oral</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        Oral presentation for EMANet</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Nov 1, 2019</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/iccv19-emanet-seg/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/iccv19-emanet-seg/featured_hu9697392da5f15752eba3cc9c3d5fcfba_326134_a8ee754e23f7d202928bfee741e7e101.webp" height="655" width="655" alt="Expectation Maximization Attention Networks for Semantic Segmentation">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Expectation Maximization Attention Networks for Semantic Segmentation</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We formulate the attention mechanism into an expectation-maximization manner and iteratively estimate a much more compact set of bases upon which the attention maps are computed.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jul 22, 2019</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/miccai19-r2net-destreak/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/miccai19-r2net-destreak/featured_hudf37fe5dd93a0851274cb8b6242e8abb_83498_bb0ba89bd3468194d54d149468d2662f.webp" height="655" width="655" alt="R^2 Net Recurrent and Recursive Network for Sparse View CT Artifacts Removal">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">R^2 Net Recurrent and Recursive Network for Sparse View CT Artifacts Removal</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We propose a novel neural network architecture to reduce streak artifacts generated in sparse-view 2D Cone Beam Computed To-mography (CBCT) image reconstruction.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jun 19, 2019</p>
    </div>
  </div>
</a>

  
  








<a href="/new_page/publication/eccv18-rescan-derain/"  class="mb-5">
  <div class="md:flex">
    <div class="md:flex-shrink-0 overflow-hidden">
      
        
        
      <img class="h-48 w-full object-cover md:w-48 hover:scale-125 transition duration-500 cursor-pointer object-cover" loading="lazy" src="/new_page/publication/eccv18-rescan-derain/featured_hu50977331c69894a416aa004775fc30b5_546954_6377088a99e07ccadaac1471ab3f4135.webp" height="655" width="655" alt="Recurrent Squeeze-and-Excitation Net for Single Image Deraining">
      
    </div>
    <div class="p-8">
      <div class="uppercase tracking-wide text-md text-primary-700 dark:text-primary-200 font-semibold">Recurrent Squeeze-and-Excitation Net for Single Image Deraining</div>
      <p class="block mt-1 text-sm leading-tight font-medium text-black dark:text-white">
        We propose a novel deep network architecture based on deep convolutional and recurrent neural networksfor single image deraining.</p>
      <p class="mt-2 text-gray-500 dark:text-gray-400 text-sm">Jul 19, 2018</p>
    </div>
  </div>
</a>

  

  </div>


  


</div>


    </div>
    <div class="page-footer">
      <footer class="container mx-auto flex flex-col justify-items-center text-sm leading-6 mt-24 mb-4 text-slate-700 dark:text-slate-200">

  












  
  
  
  
  














  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by text-center">
     2025 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by text-center">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a>  the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>

</footer>

    </div>

    
    











  </body>
</html>
